{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl5spGj0sP3n",
        "outputId": "915379d5-91da-4c2d-c9a1-6f189621ddeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import current_timestamp, date_format, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "from typing import List\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração do logger\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "XV_BmR5gsiPl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "APP_NAME = \"tb_plan_attributes\"  # Alterar\n",
        "\n",
        "FILE_PATH = f\"/content/drive/My Drive/projetos/Projeto Integrador/dataset/bronze/{APP_NAME}/\"\n",
        "OUTPUT_PATH = f\"/content/drive/My Drive/projetos/Projeto Integrador/dataset/silver/{APP_NAME}/\"\n",
        "TABLE_NAME = \"tb_silver_plan_attributes\" # Alterar\n",
        "\n",
        "COLUMNS: List[str] = [\n",
        "    \"PlanId\",\n",
        "    \"BusinessYear\",\n",
        "    \"IssuerId\",\n",
        "    \"AVCalculatorOutputNumber\",\n",
        "    \"MetalLevel\",\n",
        "    \"MarketCoverage\",\n",
        "    \"PlanMarketingName\",\n",
        "    \"StateCode\"\n",
        "    ] # Alterar"
      ],
      "metadata": {
        "id": "U8L0OrgzskUf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_info(message: str):\n",
        "    \"\"\"Função para simular o comportamento do logger.info\"\"\"\n",
        "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"{current_time} - INFO - {message}\")\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Cria e retorna uma sessão Spark.\"\"\"\n",
        "    log_info(\"Criando sessão Spark\")\n",
        "    return SparkSession.builder.appName(\"LoggerDemo\").getOrCreate()\n",
        "\n",
        "def define_silver_schema() -> StructType:\n",
        "    \"\"\"Define e retorna o schema para a tabela silver.\"\"\"\n",
        "    log_info(\"Definindo schema para a tabela silver\")\n",
        "    return StructType([\n",
        "        StructField(col, StringType(), True) for col in COLUMNS\n",
        "    ] + [\n",
        "        StructField(\"ingestDate\", TimestampType(), False),\n",
        "        StructField(\"partitionDate\", StringType(), False)\n",
        "    ])\n",
        "\n",
        "def read_and_process_data(spark: SparkSession, file_path: str, columns: List[str]) -> DataFrame:\n",
        "    \"\"\"Lê os dados do Parquet e processa para o formato desejado.\"\"\"\n",
        "    log_info(f\"Lendo e processando dados de {file_path}\")\n",
        "    df = spark.read.parquet(file_path)\n",
        "    return df.select(*columns).distinct() \\\n",
        "             .withColumn(\"ingestDate\", current_timestamp()) \\\n",
        "             .withColumn(\"partitionDate\", date_format(current_timestamp(), \"yyyyMMdd\"))\n",
        "\n",
        "def prepare_silver_data(df: DataFrame, schema: StructType) -> DataFrame:\n",
        "    \"\"\"Prepara os dados para o formato silver.\"\"\"\n",
        "    log_info(\"Preparando dados para o formato silver\")\n",
        "    return df.select([\n",
        "        col(c).cast(\"string\") for c in COLUMNS\n",
        "    ] + [\n",
        "        col(\"ingestDate\"),\n",
        "        col(\"partitionDate\")\n",
        "    ]).select(schema.fieldNames())\n",
        "\n",
        "def save_as_parquet(df: DataFrame, output_path: str):\n",
        "    \"\"\"Salva o DataFrame como arquivo Parquet no Google Drive.\"\"\"\n",
        "    log_info(f\"Salvando dados como Parquet em {output_path}\")\n",
        "    df.write \\\n",
        "      .mode(\"overwrite\") \\\n",
        "      .partitionBy(\"partitionDate\") \\\n",
        "      .parquet(output_path)"
      ],
      "metadata": {
        "id": "jeVfNzeCswHO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        spark = create_spark_session()\n",
        "        silver_schema = define_silver_schema()\n",
        "\n",
        "        df_selected = read_and_process_data(spark, FILE_PATH, COLUMNS)\n",
        "        df_silver = prepare_silver_data(df_selected, silver_schema)\n",
        "\n",
        "        save_as_parquet(df_silver, OUTPUT_PATH)\n",
        "\n",
        "        log_info(f\"Dados salvos como Parquet em: {OUTPUT_PATH}\")\n",
        "\n",
        "        # Verificar se o arquivo foi salvo corretamente\n",
        "        saved_df = spark.read.parquet(OUTPUT_PATH)\n",
        "        log_info(\"Schema dos dados salvos:\")\n",
        "        saved_df.printSchema()\n",
        "        log_info(\"Primeiras 5 linhas dos dados salvos:\")\n",
        "        saved_df.show(5)\n",
        "        log_info(f\"Contagem de registros salvos: {saved_df.count()}, na tabela: {TABLE_NAME}\")\n",
        "        log_info(\"Processo concluído com sucesso\")\n",
        "\n",
        "    except Exception as e:\n",
        "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{current_time} - ERROR - Erro durante a execução: {str(e)}\")"
      ],
      "metadata": {
        "id": "CMd6ePHIsx0e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51b9NwUCszIz",
        "outputId": "63cf275a-5173-41ca-b880-43621a9d3a78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-11 14:58:36 - INFO - Criando sessão Spark\n",
            "2025-03-11 14:58:36 - INFO - Definindo schema para a tabela silver\n",
            "2025-03-11 14:58:36 - INFO - Lendo e processando dados de /content/drive/My Drive/projetos/Projeto Integrador/dataset/bronze/tb_plan_attributes/\n",
            "2025-03-11 14:58:37 - INFO - Preparando dados para o formato silver\n",
            "2025-03-11 14:58:37 - INFO - Salvando dados como Parquet em /content/drive/My Drive/projetos/Projeto Integrador/dataset/silver/tb_plan_attributes/\n",
            "2025-03-11 14:58:41 - INFO - Dados salvos como Parquet em: /content/drive/My Drive/projetos/Projeto Integrador/dataset/silver/tb_plan_attributes/\n",
            "2025-03-11 14:58:41 - INFO - Schema dos dados salvos:\n",
            "root\n",
            " |-- PlanId: string (nullable = true)\n",
            " |-- BusinessYear: string (nullable = true)\n",
            " |-- IssuerId: string (nullable = true)\n",
            " |-- AVCalculatorOutputNumber: string (nullable = true)\n",
            " |-- MetalLevel: string (nullable = true)\n",
            " |-- MarketCoverage: string (nullable = true)\n",
            " |-- PlanMarketingName: string (nullable = true)\n",
            " |-- StateCode: string (nullable = true)\n",
            " |-- ingestDate: timestamp (nullable = true)\n",
            " |-- partitionDate: integer (nullable = true)\n",
            "\n",
            "2025-03-11 14:58:41 - INFO - Primeiras 5 linhas dos dados salvos:\n",
            "+-----------------+------------+--------+------------------------+----------+------------------+--------------------+---------+--------------------+-------------+\n",
            "|           PlanId|BusinessYear|IssuerId|AVCalculatorOutputNumber|MetalLevel|    MarketCoverage|   PlanMarketingName|StateCode|          ingestDate|partitionDate|\n",
            "+-----------------+------------+--------+------------------------+----------+------------------+--------------------+---------+--------------------+-------------+\n",
            "|73836AK0750002-00|        2016|   73836|             0.816846073|      Gold|        Individual|Be Prosperous (Se...|       AK|2025-03-11 14:58:...|     20250311|\n",
            "|73836AK0840001-01|        2016|   73836|             0.717218816|    Silver|        Individual|Moda Health Selec...|       AK|2025-03-11 14:58:...|     20250311|\n",
            "|16842FL0070114-02|        2016|   16842|                       1|    Silver|        Individual|BlueOptions Every...|       FL|2025-03-11 14:58:...|     20250311|\n",
            "|27357FL1850056-01|        2016|   27357|             0.612584233|    Bronze|SHOP (Small Group)|Health First Bron...|       FL|2025-03-11 14:58:...|     20250311|\n",
            "|30252FL0070003-02|        2016|   30252|                       1|    Bronze|        Individual|  MyBlue Bronze 1602|       FL|2025-03-11 14:58:...|     20250311|\n",
            "+-----------------+------------+--------+------------------------+----------+------------------+--------------------+---------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "2025-03-11 14:58:43 - INFO - Contagem de registros salvos: 77316\n",
            "2025-03-11 14:58:43 - INFO - Processo concluído com sucesso\n"
          ]
        }
      ]
    }
  ]
}