# -*- coding: utf-8 -*-
"""tb_silver_plan_crosswalk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118bc6cdDE8yfV0GOTM6XPuT70HHVJL-h
"""

from datetime import datetime
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import current_timestamp, date_format, col, lit, when, lit, coalesce, regexp_replace
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType

from typing import List
from functools import reduce
from google.colab import drive

drive.mount('/content/drive')

# Configuração do logger
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__)

# Configurações
APP_NAMES = ["tb_plan_crosswalk", "tb_plan_id_crosswalk"]
BASE_PATH = "/content/drive/My Drive/projetos/Projeto Integrador/dataset/"
OUTPUT_PATH = f"{BASE_PATH}silver/tb_plan_crosswalk/"
TABLE_NAME = "tb_silver_plan_crosswalk"

COLUMNS: List[str] = [
    "*"
] # Alterar

def log_info(message: str):
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{current_time} - INFO - {message}")

def log_error(message: str):
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"{current_time} - ERROR - {message}")

def create_spark_session():
    log_info("Criando sessão Spark")
    spark = SparkSession.builder.appName("UnifyServiceArea").getOrCreate()
    spark.conf.set("spark.sql.parquet.mergeSchema", "false")
    spark.conf.set("spark.sql.files.maxPartitionBytes", "128m")
    spark.conf.set("spark.sql.files.openCostInBytes", "134217728")
    spark.conf.set("spark.sql.broadcastTimeout", "3600")
    return spark

def union_dataframes(df1: DataFrame, df2: DataFrame) -> DataFrame:
    return df1.unionByName(df2, allowMissingColumns=True)

def read_and_process_data(spark: SparkSession, app_names: List[str], base_path: str) -> DataFrame:
    dfs = []
    for app_name in app_names:
        file_path = f"{base_path}bronze/{app_name}/"
        log_info(f"Tentando ler dados de {file_path}")
        try:
            df = spark.read.parquet(file_path)
            log_info(f"Dados lidos com sucesso de {file_path}")
            df_processed = df.select("*").distinct() \
                            .withColumn("ingestDate", current_timestamp()) \
                            .withColumn("partitionDate", date_format(current_timestamp(), "yyyyMMdd")) \
                            .withColumn("source", lit(app_name))
            dfs.append(df_processed)
        except Exception as e:
            log_error(f"Erro ao ler dados de {file_path}: {str(e)}")
            continue

    if not dfs:
        raise ValueError("Nenhum dado foi lido com sucesso")

    return reduce(union_dataframes, dfs)

def prepare_silver_data(df: DataFrame) -> DataFrame:
    log_info("Preparando dados para o formato silver")
    columns = df.columns
    return df.select([
        col(c).cast("string") if c not in ["ingestDate", "partitionDate", "source"] else col(c)
        for c in columns
    ])

def save_as_parquet(df: DataFrame, output_path: str):
    log_info(f"Salvando dados como Parquet em {output_path}")
    df.write \
      .mode("overwrite") \
      .partitionBy("partitionDate") \
      .parquet(output_path)

def refine_dataframe(df):
    # Função para remover o ano do nome da coluna
    def remove_year(column_name):
        return column_name.split('_')[0]

    # Função para extrair o ano do nome da coluna
    def extract_year(column_name):
        parts = column_name.split('_')
        return parts[-1] if len(parts) > 1 and (parts[-1].isdigit() or parts[-1].startswith('AgeOff')) else None

    # Identificar colunas únicas (removendo o ano)
    unique_columns = set(remove_year(c) for c in df.columns if '_20' in c)

    # Criar uma expressão para cada coluna única
    select_expr = []
    year_expr = []

    for unique_col in unique_columns:
        # Encontrar todas as colunas correspondentes (com diferentes anos)
        matching_columns = [c for c in df.columns if remove_year(c) == unique_col]

        if len(matching_columns) == 1:
            # Se há apenas uma coluna, use-a diretamente
            select_expr.append(col(matching_columns[0]).alias(unique_col))
            year = extract_year(matching_columns[0])
            if year:
                year_expr.append(when(col(matching_columns[0]).isNotNull(), lit(year)))
        else:
            # Se há múltiplas colunas, use coalesce para pegar o primeiro valor não nulo
            coalesce_expr = when(col(matching_columns[0]).isNotNull(), col(matching_columns[0]))
            year_when_expr = when(col(matching_columns[0]).isNotNull(), lit(extract_year(matching_columns[0])))

            for c in matching_columns[1:]:
                coalesce_expr = coalesce_expr.when(col(c).isNotNull(), col(c))
                year = extract_year(c)
                if year:
                    year_when_expr = year_when_expr.when(col(c).isNotNull(), lit(year))

            select_expr.append(coalesce_expr.alias(unique_col))
            year_expr.append(year_when_expr)

    # Adicionar a expressão para a coluna 'Year'
    if year_expr:
        select_expr.append(coalesce(*year_expr).alias('Year'))

    # Adicionar colunas que não têm anos e colunas de metadados
    other_columns = [c for c in df.columns if '_20' not in c]
    select_expr.extend([col(c) for c in other_columns])

    # Aplicar a seleção
    df_refined = df.select(*select_expr)

    return df_refined

def main():
    try:
        spark = create_spark_session()

        df_combined = read_and_process_data(spark, APP_NAMES, BASE_PATH)
        df_silver = prepare_silver_data(df_combined)

        # Refinar o DataFrame
        df_refined = refine_dataframe(df_silver)

        save_as_parquet(df_refined, OUTPUT_PATH)

        log_info(f"Dados salvos como Parquet em: {OUTPUT_PATH}")

        # Verificar se o arquivo foi salvo corretamente
        saved_df = spark.read.parquet(OUTPUT_PATH)
        log_info("Schema dos dados salvos:")
        saved_df.printSchema()
        log_info("Primeiras 5 linhas dos dados salvos:")
        saved_df.show(5)
        log_info(f"Contagem de registros salvos: {saved_df.count()}, na tabela: {TABLE_NAME}")
        log_info("Processo concluído com sucesso")

    except Exception as e:
        log_error(f"Erro durante a execução: {str(e)}")

if __name__ == "__main__":
    main()