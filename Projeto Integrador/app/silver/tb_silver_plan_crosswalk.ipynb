{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhd8SngM7lPM",
        "outputId": "b9fbc9a9-1744-4d87-fdca-8b704264edfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import current_timestamp, date_format, col, lit, when, explode, array, struct, explode, expr, coalesce\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
        "\n",
        "from typing import List\n",
        "from functools import reduce\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração do logger\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "# logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "w8--Fcks7t-y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurações\n",
        "APP_NAMES = [\"tb_plan_crosswalk\", \"tb_plan_id_crosswalk\"]\n",
        "BASE_PATH = \"/content/drive/My Drive/projetos/Projeto Integrador/dataset/\"\n",
        "OUTPUT_PATH = f\"{BASE_PATH}silver/tb_plan_crosswalk/\"\n",
        "TABLE_NAME = \"tb_silver_plan_crosswalk\"\n",
        "\n",
        "COLUMNS: List[str] = [\n",
        "    \"*\"\n",
        "] # Alterar"
      ],
      "metadata": {
        "id": "5dwL7yjz7xOb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_info(message: str):\n",
        "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"{current_time} - INFO - {message}\")\n",
        "\n",
        "def log_error(message: str):\n",
        "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"{current_time} - ERROR - {message}\")\n",
        "\n",
        "def create_spark_session():\n",
        "    log_info(\"Criando sessão Spark\")\n",
        "    spark = SparkSession.builder.appName(\"UnifyServiceArea\").getOrCreate()\n",
        "    spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
        "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"128m\")\n",
        "    spark.conf.set(\"spark.sql.files.openCostInBytes\", \"134217728\")\n",
        "    spark.conf.set(\"spark.sql.broadcastTimeout\", \"3600\")\n",
        "    return spark\n",
        "\n",
        "def union_dataframes(df1: DataFrame, df2: DataFrame) -> DataFrame:\n",
        "    return df1.unionByName(df2, allowMissingColumns=True)\n",
        "\n",
        "def read_and_process_data(spark: SparkSession, app_names: List[str], base_path: str) -> DataFrame:\n",
        "    dfs = []\n",
        "    for app_name in app_names:\n",
        "        file_path = f\"{base_path}bronze/{app_name}/\"\n",
        "        log_info(f\"Tentando ler dados de {file_path}\")\n",
        "        try:\n",
        "            df = spark.read.parquet(file_path)\n",
        "            log_info(f\"Dados lidos com sucesso de {file_path}\")\n",
        "            df_processed = df.select(\"*\").distinct() \\\n",
        "                            .withColumn(\"ingestDate\", current_timestamp()) \\\n",
        "                            .withColumn(\"partitionDate\", date_format(current_timestamp(), \"yyyyMMdd\")) \\\n",
        "                            .withColumn(\"source\", lit(app_name))\n",
        "            dfs.append(df_processed)\n",
        "        except Exception as e:\n",
        "            log_error(f\"Erro ao ler dados de {file_path}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"Nenhum dado foi lido com sucesso\")\n",
        "\n",
        "    return reduce(union_dataframes, dfs)\n",
        "\n",
        "def prepare_silver_data(df: DataFrame) -> DataFrame:\n",
        "    log_info(\"Preparando dados para o formato silver\")\n",
        "    columns = df.columns\n",
        "    return df.select([\n",
        "        col(c).cast(\"string\") if c not in [\"ingestDate\", \"partitionDate\", \"source\"] else col(c)\n",
        "        for c in columns\n",
        "    ])\n",
        "\n",
        "def save_as_parquet(df: DataFrame, output_path: str):\n",
        "    log_info(f\"Salvando dados como Parquet em {output_path}\")\n",
        "    df.write \\\n",
        "      .mode(\"overwrite\") \\\n",
        "      .partitionBy(\"partitionDate\") \\\n",
        "      .parquet(output_path)"
      ],
      "metadata": {
        "id": "J74UZ3mA7xFd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refine_dataframe(df):\n",
        "    def remove_year(column_name):\n",
        "        return column_name.split('_')[0]\n",
        "\n",
        "    def extract_year(column_name):\n",
        "        parts = column_name.split('_')\n",
        "        return parts[-1] if len(parts) > 1 and parts[-1].isdigit() else None\n",
        "\n",
        "    # Identificar colunas únicas e seus anos\n",
        "    column_years = [(c, extract_year(c)) for c in df.columns if '_20' in c]\n",
        "    unique_columns = set(remove_year(c) for c, _ in column_years)\n",
        "\n",
        "    # Criar expressões para cada coluna única\n",
        "    select_expr = []\n",
        "    year_columns = []\n",
        "\n",
        "    for unique_col in unique_columns:\n",
        "        matching_columns = [(c, y) for c, y in column_years if remove_year(c) == unique_col]\n",
        "\n",
        "        # Criar uma expressão coalesce para cada coluna única\n",
        "        coalesce_expr = [when(col(c).isNotNull(), struct(lit(y).alias(\"year\"), col(c).alias(\"value\")))\n",
        "                         for c, y in matching_columns]\n",
        "        select_expr.append(coalesce(*coalesce_expr).alias(f\"{unique_col}_temp\"))\n",
        "\n",
        "        # Adicionar expressão para o ano\n",
        "        year_columns.append(f\"{unique_col}_temp.year\")\n",
        "\n",
        "    # Adicionar colunas que não têm anos\n",
        "    other_columns = [c for c in df.columns if '_20' not in c]\n",
        "    select_expr.extend([col(c) for c in other_columns])\n",
        "\n",
        "    # Aplicar a seleção e adicionar a coluna YEAR\n",
        "    df_refined = df.select(*select_expr, *other_columns)\n",
        "    df_refined = df_refined.withColumn(\"YEAR\", coalesce(*year_columns))\n",
        "\n",
        "    # Extrair os valores das estruturas e renomear as colunas\n",
        "    for unique_col in unique_columns:\n",
        "        df_refined = df_refined.withColumn(unique_col, col(f\"{unique_col}_temp.value\"))\n",
        "        df_refined = df_refined.drop(f\"{unique_col}_temp\")\n",
        "\n",
        "    return df_refined"
      ],
      "metadata": {
        "id": "IbiAUd__gcGo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        spark = create_spark_session()\n",
        "\n",
        "        df_combined = read_and_process_data(spark, APP_NAMES, BASE_PATH)\n",
        "        df_silver = prepare_silver_data(df_combined)\n",
        "\n",
        "        # Refinar o DataFrame\n",
        "        df_refined = refine_dataframe(df_silver)\n",
        "\n",
        "        # Remover colunas duplicadas (se houver)\n",
        "        df_final = df_refined.select(list(dict.fromkeys(df_refined.columns)))\n",
        "\n",
        "        # Continuar com o salvamento e outras operações...\n",
        "        save_as_parquet(df_final, OUTPUT_PATH)\n",
        "\n",
        "        log_info(f\"Dados salvos como Parquet em: {OUTPUT_PATH}\")\n",
        "\n",
        "        # Verificar se o arquivo foi salvo corretamente\n",
        "        saved_df = spark.read.parquet(OUTPUT_PATH)\n",
        "        log_info(\"Schema dos dados salvos:\")\n",
        "        saved_df.printSchema()\n",
        "        log_info(\"Primeiras 5 linhas dos dados salvos:\")\n",
        "        saved_df.show(5)\n",
        "        log_info(f\"Contagem de registros salvos: {saved_df.count()}, na tabela: {TABLE_NAME}\")\n",
        "        log_info(\"Processo concluído com sucesso\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Erro durante a execução: {str(e)}\")"
      ],
      "metadata": {
        "id": "oPIvtrl87w_N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDtAXVHu7w5V",
        "outputId": "012adbde-88d0-40e4-9e8b-5ffbf994841a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-11 19:31:36 - INFO - Criando sessão Spark\n",
            "2025-03-11 19:31:36 - INFO - Tentando ler dados de /content/drive/My Drive/projetos/Projeto Integrador/dataset/bronze/tb_plan_crosswalk/\n",
            "2025-03-11 19:31:36 - INFO - Dados lidos com sucesso de /content/drive/My Drive/projetos/Projeto Integrador/dataset/bronze/tb_plan_crosswalk/\n",
            "2025-03-11 19:31:36 - INFO - Tentando ler dados de /content/drive/My Drive/projetos/Projeto Integrador/dataset/bronze/tb_plan_id_crosswalk/\n",
            "2025-03-11 19:31:36 - INFO - Dados lidos com sucesso de /content/drive/My Drive/projetos/Projeto Integrador/dataset/bronze/tb_plan_id_crosswalk/\n",
            "2025-03-11 19:31:37 - INFO - Preparando dados para o formato silver\n",
            "2025-03-11 19:31:38 - INFO - Salvando dados como Parquet em /content/drive/My Drive/projetos/Projeto Integrador/dataset/silver/tb_plan_crosswalk/\n",
            "2025-03-11 19:31:56 - INFO - Dados salvos como Parquet em: /content/drive/My Drive/projetos/Projeto Integrador/dataset/silver/tb_plan_crosswalk/\n",
            "2025-03-11 19:31:56 - INFO - Schema dos dados salvos:\n",
            "root\n",
            " |-- ChildAdultOnly_AgeOff2015: string (nullable = true)\n",
            " |-- CrosswalkLevel: string (nullable = true)\n",
            " |-- DentalPlan: string (nullable = true)\n",
            " |-- FIPSCode: string (nullable = true)\n",
            " |-- IssuerID_AgeOff2015: string (nullable = true)\n",
            " |-- MetalLevel_AgeOff2015: string (nullable = true)\n",
            " |-- MultistatePlan_AgeOff2015: string (nullable = true)\n",
            " |-- ReasonForCrosswalk: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- ZipCode: string (nullable = true)\n",
            " |-- ingestDate: timestamp (nullable = true)\n",
            " |-- source: string (nullable = true)\n",
            " |-- IssuerID_AgeOff2016: string (nullable = true)\n",
            " |-- MultistatePlan_AgeOff2016: string (nullable = true)\n",
            " |-- MetalLevel_AgeOff2016: string (nullable = true)\n",
            " |-- ChildAdultOnly_AgeOff2016: string (nullable = true)\n",
            " |-- YEAR: string (nullable = true)\n",
            " |-- PlanID: string (nullable = true)\n",
            " |-- AgeOffPlanID: string (nullable = true)\n",
            " |-- MultistatePlan: string (nullable = true)\n",
            " |-- ChildAdultOnly: string (nullable = true)\n",
            " |-- IssuerID: string (nullable = true)\n",
            " |-- MetalLevel: string (nullable = true)\n",
            " |-- partitionDate: integer (nullable = true)\n",
            "\n",
            "2025-03-11 19:31:56 - INFO - Primeiras 5 linhas dos dados salvos:\n",
            "+-------------------------+--------------+----------+--------+-------------------+---------------------+-------------------------+------------------+-----+-------+--------------------+--------------------+-------------------+-------------------------+---------------------+-------------------------+----+--------------+--------------+--------------+--------------+--------+----------+-------------+\n",
            "|ChildAdultOnly_AgeOff2015|CrosswalkLevel|DentalPlan|FIPSCode|IssuerID_AgeOff2015|MetalLevel_AgeOff2015|MultistatePlan_AgeOff2015|ReasonForCrosswalk|State|ZipCode|          ingestDate|              source|IssuerID_AgeOff2016|MultistatePlan_AgeOff2016|MetalLevel_AgeOff2016|ChildAdultOnly_AgeOff2016|YEAR|        PlanID|  AgeOffPlanID|MultistatePlan|ChildAdultOnly|IssuerID|MetalLevel|partitionDate|\n",
            "+-------------------------+--------------+----------+--------+-------------------+---------------------+-------------------------+------------------+-----+-------+--------------------+--------------------+-------------------+-------------------------+---------------------+-------------------------+----+--------------+--------------+--------------+--------------+--------+----------+-------------+\n",
            "|                     NULL|             0|         N|   41069|               NULL|                 NULL|                     NULL|                 0|   OR|  00000|2025-03-11 19:31:...|tb_plan_id_crosswalk|              00000|                        X|                    X|                        X|2015|10091OR0360009|00000XX0000000|             N|             0|   10091|      Gold|     20250311|\n",
            "|                     NULL|             1|         N|   41033|               NULL|                 NULL|                     NULL|                 6|   OR|  00000|2025-03-11 19:31:...|tb_plan_id_crosswalk|              00000|                        X|                    X|                        X|2015|10091OR0380014|00000XX0000000|             N|             0|   10091|    Silver|     20250311|\n",
            "|                     NULL|             5|         Y|   31149|               NULL|                 NULL|                     NULL|                 5|   NE|  00000|2025-03-11 19:31:...|tb_plan_id_crosswalk|              00000|                        X|                    X|                        X|2015|10739NE0010009|00000XX0000000|             N|             0|   10739|       Low|     20250311|\n",
            "|                     NULL|             0|         N|   56043|               NULL|                 NULL|                     NULL|                 0|   WY|  00000|2025-03-11 19:31:...|tb_plan_id_crosswalk|              00000|                        X|                    X|                        X|2015|11269WY0170006|00000XX0000000|             N|             0|   11269|    Bronze|     20250311|\n",
            "|                     NULL|             0|         N|   56029|               NULL|                 NULL|                     NULL|                 0|   WY|  00000|2025-03-11 19:31:...|tb_plan_id_crosswalk|              00000|                        X|                    X|                        X|2015|11269WY0170013|00000XX0000000|             N|             0|   11269|    Bronze|     20250311|\n",
            "+-------------------------+--------------+----------+--------+-------------------+---------------------+-------------------------+------------------+-----+-------+--------------------+--------------------+-------------------+-------------------------+---------------------+-------------------------+----+--------------+--------------+--------------+--------------+--------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "2025-03-11 19:31:59 - INFO - Contagem de registros salvos: 282510, na tabela: tb_silver_plan_crosswalk\n",
            "2025-03-11 19:31:59 - INFO - Processo concluído com sucesso\n"
          ]
        }
      ]
    }
  ]
}